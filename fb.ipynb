{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from webdriver_manager.microsoft import EdgeChromiumDriverManager\n",
    "from webdriver_manager.opera import OperaDriverManager\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_words = ['國安法','動態清零','黑警','疫苗猶豫','封關計劃','疫苗打死人','打針後免疫力下降','烏克蘭孕婦演員','白化病棕熊','國台辦 台灣人不是中國人 沒有資格對六四說三道四','鄧麗琼被捕','鐵腕清零','疫苗政策','佩洛西丈夫 酒駕','環保少女 通俄','港人移居','人權','去中國化','論文抄襲 (林耕仁)','民主進步黨','中國國民黨','縱容犯法','大陸間諜']\n",
    "key_words = ['共軍政變','迴力鏢','二舅 精神內耗','學生 暴動','唐山燒烤店','港區國安法','國旗國徽條例','七一刺警','獨裁','侵略','人權']\n",
    "key_words = ['骨質疏鬆','阿司匹林','致癌','麥苗汁','亞硝酸鹽','喝酒臉紅代謝好','睡蓮治癌症','減肥改善膽固醇','高危險群','骨鬆骨折','慢性疾病','胃癌','糖尿病','味精致癌','高血壓']\n",
    "key_words = ['去台化','wifi危害','盐水漱口防病毒','猴痘','化痰藥NAC','Pcr診所'] #化痰藥NAC （only 2022） Pcr診所 （only 2022）\n",
    "key_words = ['葉黃素近視','吸氫療法','飲酒預防新冠','連花清瘟肝損傷','超級真菌','信號輻射','黃豆煮水退燒']\n",
    "key_words = ['俄亥俄州列車','澤連斯基替身','蔡天鳳碎屍案','狗舔喪命','武漢抗議','中國氣球','中國製造病毒','糖尿病福音','電子煙健康','病毒唔會喺熱帶地區傳播']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = ChromeService(executable_path=ChromeDriverManager().install())\n",
    "\n",
    "prefs = {\n",
    "    'profile.default_content_setting_values':\n",
    "        {\n",
    "            'notifications': 2\n",
    "        }\n",
    "}\n",
    "\n",
    "driver = webdriver.Chrome(service=service)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"https://www.facebook.com/\")\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#email \n",
    "context = driver.find_element(by=By.ID,value='email')\n",
    "context.send_keys(\"<account>\") \n",
    "time.sleep(0.5)\n",
    "#password \n",
    "context = driver.find_element(by=By.ID,value='pass')\n",
    "context.send_keys(\"<password>\") \n",
    "time.sleep(0.5)\n",
    "login_button = driver.find_element(by=By.NAME,value='login')\n",
    "login_button.click()\n",
    "time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "                    \n",
    "# clean the date-data\n",
    "import re\n",
    "from datetime import date\n",
    "import datetime\n",
    "def to_date(date_str):\n",
    "    toyear = '2023年'\n",
    "    today = date.today()\n",
    "    d0 = today.strftime(\"%Y年%m月%d日\")\n",
    "    if(re.fullmatch(r'\\d+年\\d+月\\d+日',date_str)):\n",
    "        return date_str\n",
    "    elif(re.fullmatch(r'\\d+月\\d+日',date_str)):\n",
    "        return toyear+date_str\n",
    "    elif(re.fullmatch(r'\\d+月\\d+日\\d+:\\d+',date_str)):\n",
    "        return toyear+re.findall(r'\\d+月\\d+日',date_str)[0]\n",
    "    elif(re.fullmatch(r'\\d+天',date_str)):\n",
    "        n = int(re.findall(r'\\d+',date_str)[0])\n",
    "        return ((datetime.datetime.now())-datetime.timedelta(days=n)).strftime(\"%Y年%m月%d日\")\n",
    "    elif(re.fullmatch(r'\\d+小时',date_str)):\n",
    "        n = int(re.findall(r'\\d+',date_str)[0])\n",
    "        return ((datetime.datetime.now())-datetime.timedelta(hours=n)).strftime(\"%Y年%m月%d日\")\n",
    "    elif(re.fullmatch(r'\\d+分钟',date_str)):\n",
    "        n = int(re.findall(r'\\d+',date_str)[0])\n",
    "        return ((datetime.datetime.now())-datetime.timedelta(minutes=n)).strftime(\"%Y年%m月%d日\")\n",
    "    else:\n",
    "        return date_str\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "\n",
    "'''\n",
    "Iterate to get all retweets\n",
    "\n",
    "node_clik: the node to open the reposts\n",
    "parent_url: the parent url\n",
    "deepth: the deepth of reposts\n",
    "'''\n",
    "def dfs_repost(node_clik,parent_url,deepth=0):\n",
    "    diff = '/html/body/div[1]/div/div[1]/div/div[4]'\n",
    "    diff = '/html/body/div[1]/div/div[1]/div/div[4]'+'/div/div/div[2]'*deepth+'/div/div'\n",
    "    try:\n",
    "        try:\n",
    "            t=int(''.join(node_clik.text[:-3].split(',')))\n",
    "        except ValueError:\n",
    "            return[]\n",
    "        if(t < 2 and t >800):\n",
    "            return []\n",
    "        ActionChains(driver).click(node_clik).perform()\n",
    "        # wait the content loading or wrong\n",
    "        time.sleep(6)\n",
    "        try:\n",
    "            ActionChains(driver).click(driver.find_element(by=By.XPATH,value='/html/body/div[5]/div[1]/div/div[2]/div/div/div/div[2]/div | /html/body/div[6]/div[1]/div/div[2]/div/div/div/div[2]/div')).perform()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "        diff_node = driver.find_element(by=By.XPATH,value=diff)\n",
    "        win = diff_node.find_element(by=By.XPATH,value='./div[1]/div/div[2]/div/div/div/div[3]')\n",
    "        ActionChains(driver).move_to_element(diff_node.find_element(by=By.XPATH,value='./div[1]/div/div[2]/div/div/div/div[3]')).perform()\n",
    "\n",
    "        time.sleep(1)\n",
    "        roll = diff_node.find_element(by=By.XPATH,value='./div[1]/div/div[2]/div/div/div/div[3]/div/div[3]')\n",
    "  \n",
    "        maxt= t\n",
    "        while True:\n",
    "            time.sleep(1-t/maxt)\n",
    "            try:\n",
    "                # Dynamic dragging of the slider\n",
    "                delta = t*19/maxt\n",
    "                ActionChains(driver).drag_and_drop_by_offset(roll,0,win.location['y']+win.size['height']-roll.size['height']-roll.location['y']-delta-1).perform()\n",
    "\n",
    "            except NoSuchElementException:\n",
    "                continue\n",
    "            try:\n",
    "                if(diff_node.find_elements(by=By.XPATH,value='./div[1]/div/div[2]/div/div/div/div[3]/div/div[1]/div')[-2].get_attribute('class')==''):\n",
    "                    break\n",
    "            except StaleElementReferenceException:\n",
    "                pass\n",
    "            t = max(t - 1,0)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    time.sleep(0.5)\n",
    "    # find all reposts\n",
    "    diff_node = driver.find_element(by=By.XPATH,value=diff)\n",
    "    all_reposts = diff_node.find_elements(by=By.XPATH,value='./div[1]/div/div[2]/div/div/div/div[3]/div/div[1]/div')\n",
    "\n",
    "    try:\n",
    "        all_reposts.pop()\n",
    "        all_reposts.pop()\n",
    "    except IndexError:\n",
    "        print('something wrong')\n",
    "        return []\n",
    "    reposts = []\n",
    "    for repost_node in all_reposts:\n",
    "        # get the data of each repost\n",
    "        repost_item = {}\n",
    "        try:\n",
    "            a_user = repost_node.find_element(by=By.XPATH,value='./div/div/div/div/div[2]/div/div[2]/div/div[1]/span/h3//span/a')\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "        url = repost_node.find_element(by=By.XPATH,value='./div/div/div/div/div[2]/div/div[2]/div/div[2]/span/span/span[2]/span/a | ./div/div/div/div[2]/div/div[2]/div/div[2]/span/span/a')\n",
    "        try:\n",
    "            repost_item[\"post_text\"] = repost_node.find_element(by=By.XPATH,value='./div/div/div/div/div[3]/div[1]/div/div/div/span').text\n",
    "        except NoSuchElementException:\n",
    "            repost_item[\"post_text\"]=''\n",
    "            pass\n",
    "        try:\n",
    "            repost_item[\"post_reaction_num\"] = repost_node.find_element(by=By.XPATH,value='./div/div/div/div/div[4]/div/div/div/div/div[1]/div/div[1]/div/span/div/span[2]/span/span').text\n",
    "        except NoSuchElementException:\n",
    "            repost_item[\"post_reaction_num\"] = 0\n",
    "            pass\n",
    "        try:\n",
    "            repost_item[\"post_share_comment\"] = repost_node.find_element(by=By.XPATH,value='./div/div/div/div/div[4]/div/div/div[1]/div/div[1]/div/div[last()]').text\n",
    "        except NoSuchElementException:\n",
    "            repost_item[\"post_share_comment\"] = ''\n",
    "            pass\n",
    "        repost_item[\"post_url\"]  = url.get_attribute('href').split('?')[0]\n",
    "        repost_item[\"post_time\"] = to_date(url.text)\n",
    "        repost_item[\"post_user_url\"] = a_user.get_attribute('href').split('?')[0]\n",
    "        repost_item[\"post_user_name\"] = a_user.text\n",
    "        repost_item[\"parent_url\"] = parent_url\n",
    "        repost_item[\"reposts\"] = []\n",
    "        if('分享' in repost_item[\"post_share_comment\"] and repost_item[\"post_share_comment\"] != '分享' and '用户' not in repost_item[\"post_share_comment\"]):\n",
    "            repost_item[\"reposts\"] = dfs_repost(repost_node.find_element(by=By.XPATH,value='./div/div/div/div/div[4]/div/div/div[1]/div/div[1]/div/div[last()]/div[last()]/span'),repost_item[\"post_url\"],deepth+1)\n",
    "        reposts.append(repost_item)\n",
    "\n",
    "    # close the repost\n",
    "    close = diff_node.find_element(by=By.XPATH,value='./div[1]/div/div[2]/div/div/div/div[2]')\n",
    "    close.click()\n",
    "    time.sleep(1)\n",
    "    return reposts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some topic\n",
    "key_word = key_words[0]\n",
    "driver.get('https://www.facebook.com/search/posts/?q={}'.format(key_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy loading\n",
    "while True:\n",
    "    time.sleep(0.5)\n",
    "    driver.execute_script(\"window.scrollBy(0,1500)\")\n",
    "    try:\n",
    "        if(driver.find_element(by=By.XPATH,value='/html/body/div[1]/div/div[1]/div/div[3]/div/div/div/div[1]/div[1]/div[2]/div/div/div/div/div/div[last()]').text=='已经到底啦~'):\n",
    "            break\n",
    "    except StaleElementReferenceException:\n",
    "        pass\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import json\n",
    "import uuid\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import traceback\n",
    "# get all post\n",
    "all_posts = driver.find_elements(by=By.XPATH,value='/html/body/div[1]/div/div[1]/div/div[3]/div/div/div/div[1]/div[1]/div[2]/div/div/div/div/div/div')\n",
    "\n",
    "# remove the last\n",
    "all_posts.pop() \n",
    "posts = [] \n",
    "repost_sites = ['/div[1]/div/div[2]/div/div/div/div[3]/div/div[1]/div','/div[1]/div/div[2]/div/div/div/div[3]/div/div[1]/div']\n",
    "xians = ['/div[1]/div/div[2]/div/div/div','/div[1]/div/div[2]/div/div/div']\n",
    "wins = ['/div[1]/div/div[2]/div/div/div/div[3]','/div[1]/div/div[2]/div/div/div/div[3]']\n",
    "rolls = ['/div[1]/div/div[2]/div/div/div/div[3]/div/div[3]','/div[1]/div/div[2]/div/div/div/div[3]/div/div[3]']\n",
    "closes = ['/div[1]/div/div[2]/div/div/div/div[2]','/div[1]/div/div[2]/div/div/div/div[2]']\n",
    "ends = ['/div[1]/div/div[2]/div/div/div/div[3]/div/div[1]/div','/div[1]/div/div[2]/div/div/div/div[3]/div/div[1]/div']\n",
    "for node in all_posts[:]:\n",
    "    ActionChains(driver).scroll_to_element(node).perform()\n",
    "    time.sleep(0.233)\n",
    "    try:\n",
    "        open_text = node.find_element(by=By.XPATH,value='./div/div/div/div/div/div/div/div/div/div/div[2]/div/div/div[3]/div[1]/div/div/div/span/div[last()]/div/div')\n",
    "        ActionChains(driver).scroll_to_element(open_text).perform()\n",
    "        time.sleep(0.233)\n",
    "        ActionChains(driver).click(open_text).perform()\n",
    "        time.sleep(0.233)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    try:\n",
    "        # get the data of the post\n",
    "        post_item = {}\n",
    "        a_user = node.find_element(by=By.XPATH,value='./div/div/div/div/div/div/div/div/div/div/div[2]/div/div/div[2]/div/div[2]/div/div[1]/span/h3//span/a')\n",
    "        url = node.find_element(by=By.XPATH,value='./div/div/div/div/div/div/div/div/div/div/div[2]/div/div/div[2]/div/div[2]/div/div[2]/span/span/span[2]/span/a | ./div/div/div/div/div/div/div/div/div/div/div[2]/div/div[2]/div/div[2]/div/div[2]/span/span/a')\n",
    "        reaction = node.find_element(by=By.XPATH,value='./div/div/div/div/div/div/div/div/div/div/div[2]/div/div/div[last()]/div/div/div[1]/div/div[1]')\n",
    "        post_item[\"post_url\"] = url.get_attribute('href').split('?')[0]\n",
    "        post_item[\"post_time\"] = to_date(url.text)\n",
    "        post_item[\"post_user_url\"] = a_user.get_attribute('href').split('?')[0]\n",
    "        post_item[\"post_user_name\"] = a_user.text\n",
    "        print(post_item[\"post_user_name\"])\n",
    "        post_item[\"post_text\"] = node.find_element(by=By.XPATH,value='./div/div/div/div/div/div/div/div/div/div/div[2]/div/div/div[3]/div[1]/div').text\n",
    "        post_item[\"post_reaction_num\"] = reaction.find_element(by=By.XPATH,value='./div/div[1]/div/span/div/span[1]').text\n",
    "        post_item[\"post_share_comment\"] = reaction.find_element(by=By.XPATH,value='./div/div[last()]').text\n",
    "        post_item[\"reposts\"] = []\n",
    "    except NoSuchElementException as e:\n",
    "        # traceback.print_exc()\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        text = reaction.find_element(by=By.XPATH,value='./div/div[last()]').text\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        continue\n",
    "    # if exist reposts, get the reposts\n",
    "    if('分享' in text and text != '分享'):\n",
    "        \n",
    "        try:\n",
    "            t=int(''.join(reaction.find_element(by=By.XPATH,value='./div/div[last()]/div[last()]/span').text[:-3].split(',')))\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if(t < 2 or t > 600):\n",
    "            continue\n",
    "        click_node = reaction.find_element(by=By.XPATH,value='./div/div[last()]/div[last()]/span')\n",
    "        post_item[\"reposts\"] = dfs_repost(click_node,post_item[\"post_url\"])\n",
    "        # store the post\n",
    "        with open('healthy_v4_{}.json'.format(key_word),'a',encoding='utf-8') as f:\n",
    "    # ''.join(str(uuid.uuid1()).s   plit('-'))\n",
    "            f.write(json.dumps(post_item,ensure_ascii=False))\n",
    "            f.write('\\n')\n",
    "    time.sleep(0.5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "501834c540f8b330c04c1d43ec1666643355d13f80c9caf7e1080236b3fb263f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
